{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from numpy import random\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import tweepy\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from langdetect import detect\n",
    "from langid import classify\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "from textblob.translate import Translator\n",
    "from textblob import TextBlob\n",
    "from textblob_nl import PatternTagger, PatternAnalyzer\n",
    "\n",
    "import time as t\n",
    "\n",
    "import spacy\n",
    "\n",
    "twitter_c = [0.11, 0.63, 0.95]\n",
    "pd.set_option('display.max_colwidth', -1) # normally 50\n",
    "data = 'DATA/'\n",
    "pickles = 'pickles/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_url(txt, sub = '||U||'):\n",
    "    \"\"\"Replace URLs found in a text string with ||U|| \n",
    "    (i.e. it will remove the URL from the string).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    txt : string\n",
    "        A text string that you want to parse and remove urls.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The same txt string with url's replaced.\n",
    "    \"\"\"\n",
    "    return \" \".join(re.sub(\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\", sub, txt).split())\n",
    "\n",
    "def replace_tags(txt, sub = \"||T||\"):\n",
    "    return \" \".join(re.sub('(@[a-zA-Z0-9]+)', sub, txt).split())\n",
    "\n",
    "def check_inst(txt, inst):\n",
    "    for word in txt.split():\n",
    "        if word in inst:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def nl_sent(text):\n",
    "    ''' \n",
    "    text: string in Dutch\n",
    "    returns \n",
    "    '''\n",
    "    return TextBlob(text, analyzer = PatternAnalyzer()).sentiment[0]\n",
    "\n",
    "def en_sent(text):\n",
    "    '''\n",
    "    text: string in English\n",
    "    returns: polarity sentiment of the given text. the text is translated from dutch to english\n",
    "    '''\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def remove_punct(text):\n",
    "    '''\n",
    "    removes all punctuation from a string\n",
    "    '''\n",
    "    return re.sub('([^\\sA-Za-z0-9])', '', text)\n",
    "\n",
    "def remove_small(text):\n",
    "    '''\n",
    "    removes small words of size smaller than 3\n",
    "    '''\n",
    "    return re.sub('(\\\\W*\\\\b\\\\w{1,2}\\\\b)', '', text)\n",
    "\n",
    "stopwords_l = set(stopwords.words('dutch'))\n",
    "\n",
    "def remove_stopwords(text, stopwords = stopwords_l):\n",
    "    '''\n",
    "    text: string\n",
    "    stopwords: (array-like, optional) a list containing stopwords. default list is dutch.\n",
    "    removes stopwords from the given text\n",
    "    '''\n",
    "    word_list = text.split()\n",
    "    return ' '.join([word for word in word_list if word not in stopwords])\n",
    "\n",
    "def small_tweet(text):\n",
    "    '''\n",
    "    returns true when the given text is smaller than 3 words\n",
    "    '''\n",
    "    length = len(text.split())\n",
    "    if length < 3:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def contains_text(text):\n",
    "    '''\n",
    "    if text contains text True is returned False otherwise'''\n",
    "    if re.search('([a-zA-Z0-9]+)', text):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def detect_lang(x):\n",
    "    '''\n",
    "    x: a string of text\n",
    "    returns the language and None if there is no language detected'''\n",
    "    try:\n",
    "        lang = detect(x)\n",
    "        return lang\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def sentence_checker(sentence, speller):\n",
    "    '''\n",
    "    sentence: a string\n",
    "    speller: a SpellChecker object\n",
    "    returns the same text but corrected for spelling'''\n",
    "    word_list = sentence.split()\n",
    "    for i, word in enumerate(word_list):\n",
    "        new = word_checker(word, speller)\n",
    "        word_list[i] = new\n",
    "    return ' '.join(word_list)\n",
    "\n",
    "def word_checker(word, speller):\n",
    "    '''\n",
    "    word: a string containing a single word\n",
    "    speller: a SpellChecker object\n",
    "    returns: the corrected word'''\n",
    "    if word in speller:\n",
    "        return word\n",
    "    else:\n",
    "        cor = speller.correction(word)\n",
    "#         print('5', cor)\n",
    "        return cor\n",
    "\n",
    "def sent_cat(sent, threshold = 0.05):\n",
    "    '''\n",
    "    Sets sentiment score to 1 if positive, -1 if negative and 0 if neutral\n",
    "    theshold (float) default 0.05 and -(0.05) for negative side\n",
    "    '''\n",
    "    if sent > threshold:\n",
    "        return 1\n",
    "    elif sent < -threshold:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def is_pos(x, threshold = 0.05):\n",
    "    if x > threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def is_neg(x, threshold = 0.05):\n",
    "    if x < -threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def margins(freq, margin = 0.05):\n",
    "    '''\n",
    "    freq: a integer representing the frequency of a word occuring in the text\n",
    "    margin: the margin to consider similar word frequencies default 0.05\n",
    "    returns a tuple with lower and upperbound'''\n",
    "    upper = freq*(margin+1)\n",
    "    lower = freq*(1-margin)\n",
    "    return (lower, upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First analysis of the twitter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check spelling checkers\n",
    "The spellingschecker is checked on spelling of words and whether weird things happen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year: 2014\n",
      "2014 01-01 tot 01-11.csv\n",
      "(14200, 12)\n",
      "2014 01-11 tot 01-22.csv\n",
      "(28500, 12)\n",
      "2014 01-22 tot 01-25.csv\n",
      "(33000, 12)\n",
      "2014 01-25 tot 02-08.csv\n",
      "(47300, 12)\n",
      "2014 02-09 tot 02-23.csv\n",
      "(61600, 12)\n",
      "2014 02-23 tot 03-14.csv\n",
      "(75900, 12)\n",
      "2014 03-13 tot 03-31.csv\n",
      "(92700, 12)\n",
      "2014 04-02 tot 04-19.csv\n",
      "(107000, 12)\n",
      "2014 04-19 tot 04-20.csv\n",
      "(108007, 12)\n",
      "2014 04-20 tot 05-19.csv\n",
      "(129007, 12)\n",
      "2014 05-19 tot 05-28.csv\n",
      "(135143, 12)\n",
      "2014 05-28 tot 06-14.csv\n",
      "(149443, 12)\n",
      "2014 06-15 tot 06-29.csv\n",
      "(163743, 12)\n",
      "2014 06-29 tot 07-13.csv\n",
      "(178043, 12)\n",
      "2014 07-13 tot 08-02.csv\n",
      "(192343, 12)\n",
      "2014 08-01 tot 08-25.csv\n",
      "(206643, 12)\n",
      "2014 08-25 tot 09-16.csv\n",
      "(220943, 12)\n",
      "2014 09-16 tot 10-15.csv\n",
      "(235243, 12)\n",
      "2014 10-15 tot 11-14.csv\n",
      "(249643, 12)\n",
      "2014 11-14 tot 11-26.csv\n",
      "(254843, 12)\n",
      "2014 11-26 tot 12-31.csv\n",
      "(269143, 12)\n",
      "select data\n",
      "(264710, 12)\n",
      "year: 2015\n",
      "2015 01-01 tot 01-29.csv\n",
      "(9117, 12)\n",
      "2015 01-29 tot 02-28.csv\n",
      "(23517, 12)\n",
      "2015 03-01 tot 03-27.csv\n",
      "(37817, 12)\n",
      "2015 03-27 tot 04-26.csv\n",
      "(52117, 12)\n",
      "2015 04-25 tot 08-06.csv\n",
      "(66517, 12)\n",
      "2015 08-06 tot 12-31.csv\n",
      "(80317, 12)\n",
      "select data\n",
      "(79893, 12)\n",
      "year: 2016\n",
      "2016 01-01 tot 01-03.csv\n",
      "(346, 12)\n",
      "2016 01-03 tot 07-21.csv\n",
      "(14746, 12)\n",
      "2016 07-20 tot 12-31.csv\n",
      "(28846, 12)\n",
      "select data\n",
      "(28821, 12)\n",
      "year: 2017\n",
      "2017 01-01 tot 01-29.csv\n",
      "(2451, 12)\n",
      "2017 01-28 tot 07-05.csv\n",
      "(16351, 12)\n",
      "2017 07-05 tot 12-31.csv\n",
      "(28251, 12)\n",
      "select data\n",
      "(28251, 12)\n",
      "year: 2018\n",
      "2018 01-01 tot 11-07.csv\n",
      "(22571, 12)\n",
      "2018 11-08 tot 11-08.csv\n",
      "(22608, 12)\n",
      "2018 11-08 tot 11-19.csv\n",
      "(23307, 12)\n",
      "2018 11-20 tot 11-20.csv\n",
      "(23328, 12)\n",
      "2018 11-20 tot 12-31.csv\n",
      "(25559, 12)\n",
      "select data\n",
      "(25501, 12)\n",
      "year: 2019\n",
      "2019 01-01 tot 04-17.csv\n",
      "(16531, 12)\n",
      "2019 04-17 tot 05-21.csv\n",
      "(30831, 12)\n",
      "2019 05-21 tot 06-23.csv\n",
      "(45231, 12)\n",
      "2019 06-24 tot 08-09.csv\n",
      "(59531, 12)\n",
      "2019 08-09 tot 09-09.csv\n",
      "(71295, 12)\n",
      "2019 09-09 tot 10-15.csv\n",
      "(85595, 12)\n",
      "2019 10-15 tot 11-25.csv\n",
      "(99895, 12)\n",
      "2019 11-25 tot 12-31.csv\n",
      "(114195, 12)\n",
      "select data\n",
      "(113505, 12)\n"
     ]
    }
   ],
   "source": [
    "# years of data \n",
    "years = ['2014', '2015', '2016', '2017', '2018', '2019']\n",
    "\n",
    "path = os.getcwd()\n",
    "data_folder = '15 km radius'\n",
    "\n",
    "year_data = {}\n",
    "\n",
    "for year in years:\n",
    "    print('year: '+year)\n",
    "    files = [file for file in os.listdir(path+os.sep+data_folder) if year in file] # pay attention where your data is stored\n",
    "    temp = pd.DataFrame(data = None, columns = ['date', 'username', 'to', 'replies', 'retweets', 'favorites', 'text',\n",
    "                                                'geo', 'mentions', 'hashtags', 'id', 'permalink'])\n",
    "\n",
    "    for file in files:\n",
    "        print(file)\n",
    "        temp = temp.append(pd.read_csv(path+os.sep+data_folder+os.sep+file), ignore_index = True)\n",
    "        print(temp.shape)\n",
    "\n",
    "    start = t.time()\n",
    "    print('select data')\n",
    "    tweets = temp\n",
    "    tweets = tweets.drop_duplicates()\n",
    "    print(tweets.shape)\n",
    "    year_data[year] = tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014  (264710, 12)\n",
      "2015  (79893, 12)\n",
      "2016  (28821, 12)\n",
      "2017  (28251, 12)\n",
      "2018  (25501, 12)\n",
      "2019  (113505, 12)\n"
     ]
    }
   ],
   "source": [
    "# getting the total number of tweets in each year before pre-processing\n",
    "tweets = pd.DataFrame(data = None, columns = ['date', 'username', 'to', 'replies', 'retweets', 'favorites', 'text',\n",
    "                                            'geo', 'mentions', 'hashtags', 'id', 'permalink'])\n",
    "for year in years:\n",
    "    temp = year_data[year]\n",
    "    print(year+' ', year_data[year].shape)\n",
    "    tweets = tweets.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 10\n",
    "temp = tweets.sample(n = 5000, random_state= random_state)\n",
    "temp.loc[:,'langdetect'] = temp.loc[:,'text'].apply(lambda x: detect_lang(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>23332</td>\n",
       "      <td>GOIRLE - SPRAAKBERICHT: https://is.gd/A9fO0W , Details: ...https://is.gd/9qHRTo</td>\n",
       "      <td>GOIRLE - SPRAAKBERICHT: https://is.gd/A9fO0W , details ...https://is.gd/9qHRTo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105696</td>\n",
       "      <td>@Rickk_013 komt omdat er ook weinig opzit</td>\n",
       "      <td>@Rickk_013 komt omdat er ook weinig opzij</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34292</td>\n",
       "      <td>Klopt. En de volgende ochtend weer om 6:30 uur gaat de wekker weer.</td>\n",
       "      <td>klopt En de volgende ochtend weer om 6:30 uur gaat de lekker weer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59521</td>\n",
       "      <td>SBS 6 live uitzending van Maarten. #11Stedenzwemtocht</td>\n",
       "      <td>sms 6 live uitzending van Maarten. #11Stedenzwemtocht</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258562</td>\n",
       "      <td>\"@Chris_dfwy: @DweezyKid @Sabajo_ maar alle grappen op een stok, Sherwin lai deng thoo\"dat mi no sabie , ik weet van mij</td>\n",
       "      <td>\"@Chris_dfwy: @DweezyKid @Sabajo_ maar alle grappen op een stok Sherwin lag denk thoo\"dat me nog sabie , ik weet van mij</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178310</td>\n",
       "      <td>Pff wat een gezeik hier. Does ff normaal tegen de #joden. #jodenhaat of #moslimhaat lijdt nergens toe. Gewoon geen haat!</td>\n",
       "      <td>ff wat een gezeik hier doen ff normaal tegen de #joden. #jodenhaat of #moslimhaat lijdt nergens toen Gewoon geen haat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186704</td>\n",
       "      <td>Bijkomen van een super gezellige kermisavond! Beetje liggen, een kopje koffie en genieten... http://4sq.com/UjPZ1v pic.twitter.com/X7vSfhVNHl</td>\n",
       "      <td>Bijkomen van een super gezellig kermisavond! Beetje liggen een kopje koffie en genieten... http://4sq.com/UjPZ1v pic.twitter.com/X7vSfhVNHl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96200</td>\n",
       "      <td>Vandaag al 14 maandjes met babe @PaulPPF jammer dat ik vandaag niet met je kan zijn schat heel veel plezier i love you</td>\n",
       "      <td>Vandaag al 14 maandjes met baby @PaulPPF jammer dat ik vandaag niet met je kan zijn schat heel veel plezier ik love you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5905</td>\n",
       "      <td>My favorite! Toral! #zweertsschoenen #myfavorites #loafers #shoes #studs #trendy #fashionable #spanish #luxury #panterprint @Zweerts Schoenen Oisterwijk https://www.instagram.com/p/BnMbYd0BAaX/?utm_source=ig_twitter_share&amp;igshid=1d8iqkl9fei0y …</td>\n",
       "      <td>me favorite! Toral! #zweertsschoenen #myfavorites #loafers #shoes #studs #trendy #fashionable #spanish #luxury #panterprint @Zweerts Schoenen Oisterwijk https://www.instagram.com/p/BnMbYd0BAaX/?utm_source=ig_twitter_share&amp;igshid=1d8iqkl9fei0y …</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18321</td>\n",
       "      <td>(Directe inzet: A2 5041DN 12 F : Stedekestraat 12 F Tilburg 20109 http://watiserloos.in/melding/14632409/ … #p2000</td>\n",
       "      <td>(Directe inzet A2 5041DN 12 of : Stedekestraat 12 of Tilburg 20109 http://watiserloos.in/melding/14632409/ … #p2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19776</td>\n",
       "      <td>“ @Rogier24: Wat een legendarische avond! 52-0 #EKKwalificatie #EKT2015 #LetsGoBelgrado #NederlandLetland pic.twitter.com/oktz0dukwk” #SPECO</td>\n",
       "      <td>“ @Rogier24: Wat een legendarische avond 52-0 #EKKwalificatie #EKT2015 #LetsGoBelgrado #NederlandLetland pic.twitter.com/oktz0dukwk” #SPECO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22635</td>\n",
       "      <td>@huismussen goed dat je dat eens meldt</td>\n",
       "      <td>@huismussen goed dat je dat eens geldt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58414</td>\n",
       "      <td>\" @MounirTweets: Je bent Online Online Online Online Online Online op whatsapp.... Met wie praat je? als je niet met mij praat.\"</td>\n",
       "      <td>\" @MounirTweets: Je bent Online Online Online Online Online Online op whatsapp.... Met wie praat je als je niet met mij praat.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35727</td>\n",
       "      <td>@Politie_Tilburg tip: enige vorm van terugkoppeling bij een vraag is niet verkeerd!</td>\n",
       "      <td>@Politie_Tilburg tip enige vorm van terugkoppeling bij een vraag is niet verkeerd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47284</td>\n",
       "      <td>@Leerdenken @haesebroeck @VGZ ja het is echt verschrikkelijk. Hoe vaak je formulieren opnieuw moet insturen.</td>\n",
       "      <td>@Leerdenken @haesebroeck @VGZ ja het is echt verschrikkelijk Hoe vaak je formulieren opnieuw moet insturen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106004</td>\n",
       "      <td>Ik snap wat je bedoeld Veerle, gezamenlijke plannen is ook hier elk jaar weer lastig, met een geitenboerin en een vrouw van een agrarisch medewerker als zussen en ik als boer. Nogmaals ik vind het niet erg hè. Verschil: Ik wordt niet door een bààs gepiepeld....</td>\n",
       "      <td>Ik snap wat je bedoeld Veerle, gezamenlijke plannen is ook hier elk jaar weer lastig met een geitenboerin en een vrouw van een agrarisch medewerker als zussen en ik als boer Nogmaals ik vind het niet erg hè verschil Ik wordt niet door een bààs gepiepeld....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41168</td>\n",
       "      <td>@onzetaal @liannekoops vooral leuk omdat zelfs de journalist het woord niet lijkt te kennen. Ontdekt door @MaraGelderblom</td>\n",
       "      <td>@onzetaal @liannekoops vooral leuk omdat zelfs de journalist het woord niet lijkt te kennen Ontdekt door @MaraGelderblom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>Er is op dit moment altijd wel iemand die 'Dans je de hele nacht met mij' in zijn of haar hoofd heeft.</td>\n",
       "      <td>Er is op dit moment altijd wel iemand die dans je de hele nacht met mijn in zijn of haar hoofd heeft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8464</td>\n",
       "      <td>#Bathsheba uit Antwerpen (met een #Seef uit #antwerpen) rulet roadburnfest culdesactilburg… https://www.instagram.com/p/BTH06mJB2Tn/</td>\n",
       "      <td>#Bathsheba uit Antwerpen met een #Seef uit #antwerpen) rulet roadburnfest culdesactilburg… https://www.instagram.com/p/BTH06mJB2Tn/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42349</td>\n",
       "      <td>@nynkevdploeg veel plezier daar!</td>\n",
       "      <td>@nynkevdploeg veel plezier daar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                         text  \\\n",
       "23332   GOIRLE - SPRAAKBERICHT: https://is.gd/A9fO0W , Details: ...https://is.gd/9qHRTo                                                                                                                                                                                         \n",
       "105696  @Rickk_013 komt omdat er ook weinig opzit                                                                                                                                                                                                                               \n",
       "34292   Klopt. En de volgende ochtend weer om 6:30 uur gaat de wekker weer.                                                                                                                                                                                                     \n",
       "59521   SBS 6 live uitzending van Maarten. #11Stedenzwemtocht                                                                                                                                                                                                                   \n",
       "258562  \"@Chris_dfwy: @DweezyKid @Sabajo_ maar alle grappen op een stok, Sherwin lai deng thoo\"dat mi no sabie , ik weet van mij                                                                                                                                                \n",
       "178310  Pff wat een gezeik hier. Does ff normaal tegen de #joden. #jodenhaat of #moslimhaat lijdt nergens toe. Gewoon geen haat!                                                                                                                                                \n",
       "186704  Bijkomen van een super gezellige kermisavond! Beetje liggen, een kopje koffie en genieten... http://4sq.com/UjPZ1v pic.twitter.com/X7vSfhVNHl                                                                                                                           \n",
       "96200   Vandaag al 14 maandjes met babe @PaulPPF jammer dat ik vandaag niet met je kan zijn schat heel veel plezier i love you                                                                                                                                                  \n",
       "5905    My favorite! Toral! #zweertsschoenen #myfavorites #loafers #shoes #studs #trendy #fashionable #spanish #luxury #panterprint @Zweerts Schoenen Oisterwijk https://www.instagram.com/p/BnMbYd0BAaX/?utm_source=ig_twitter_share&igshid=1d8iqkl9fei0y …                    \n",
       "18321   (Directe inzet: A2 5041DN 12 F : Stedekestraat 12 F Tilburg 20109 http://watiserloos.in/melding/14632409/ … #p2000                                                                                                                                                      \n",
       "19776   “ @Rogier24: Wat een legendarische avond! 52-0 #EKKwalificatie #EKT2015 #LetsGoBelgrado #NederlandLetland pic.twitter.com/oktz0dukwk” #SPECO                                                                                                                            \n",
       "22635   @huismussen goed dat je dat eens meldt                                                                                                                                                                                                                                  \n",
       "58414   \" @MounirTweets: Je bent Online Online Online Online Online Online op whatsapp.... Met wie praat je? als je niet met mij praat.\"                                                                                                                                        \n",
       "35727   @Politie_Tilburg tip: enige vorm van terugkoppeling bij een vraag is niet verkeerd!                                                                                                                                                                                     \n",
       "47284   @Leerdenken @haesebroeck @VGZ ja het is echt verschrikkelijk. Hoe vaak je formulieren opnieuw moet insturen.                                                                                                                                                            \n",
       "106004  Ik snap wat je bedoeld Veerle, gezamenlijke plannen is ook hier elk jaar weer lastig, met een geitenboerin en een vrouw van een agrarisch medewerker als zussen en ik als boer. Nogmaals ik vind het niet erg hè. Verschil: Ik wordt niet door een bààs gepiepeld....   \n",
       "41168   @onzetaal @liannekoops vooral leuk omdat zelfs de journalist het woord niet lijkt te kennen. Ontdekt door @MaraGelderblom                                                                                                                                               \n",
       "174     Er is op dit moment altijd wel iemand die 'Dans je de hele nacht met mij' in zijn of haar hoofd heeft.                                                                                                                                                                  \n",
       "8464    #Bathsheba uit Antwerpen (met een #Seef uit #antwerpen) rulet roadburnfest culdesactilburg… https://www.instagram.com/p/BTH06mJB2Tn/                                                                                                                                    \n",
       "42349   @nynkevdploeg veel plezier daar!                                                                                                                                                                                                                                        \n",
       "\n",
       "                                                                                                                                                                                                                                                                corrected  \n",
       "23332   GOIRLE - SPRAAKBERICHT: https://is.gd/A9fO0W , details ...https://is.gd/9qHRTo                                                                                                                                                                                     \n",
       "105696  @Rickk_013 komt omdat er ook weinig opzij                                                                                                                                                                                                                          \n",
       "34292   klopt En de volgende ochtend weer om 6:30 uur gaat de lekker weer                                                                                                                                                                                                  \n",
       "59521   sms 6 live uitzending van Maarten. #11Stedenzwemtocht                                                                                                                                                                                                              \n",
       "258562  \"@Chris_dfwy: @DweezyKid @Sabajo_ maar alle grappen op een stok Sherwin lag denk thoo\"dat me nog sabie , ik weet van mij                                                                                                                                           \n",
       "178310  ff wat een gezeik hier doen ff normaal tegen de #joden. #jodenhaat of #moslimhaat lijdt nergens toen Gewoon geen haat                                                                                                                                              \n",
       "186704  Bijkomen van een super gezellig kermisavond! Beetje liggen een kopje koffie en genieten... http://4sq.com/UjPZ1v pic.twitter.com/X7vSfhVNHl                                                                                                                        \n",
       "96200   Vandaag al 14 maandjes met baby @PaulPPF jammer dat ik vandaag niet met je kan zijn schat heel veel plezier ik love you                                                                                                                                            \n",
       "5905    me favorite! Toral! #zweertsschoenen #myfavorites #loafers #shoes #studs #trendy #fashionable #spanish #luxury #panterprint @Zweerts Schoenen Oisterwijk https://www.instagram.com/p/BnMbYd0BAaX/?utm_source=ig_twitter_share&igshid=1d8iqkl9fei0y …               \n",
       "18321   (Directe inzet A2 5041DN 12 of : Stedekestraat 12 of Tilburg 20109 http://watiserloos.in/melding/14632409/ … #p2000                                                                                                                                                \n",
       "19776   “ @Rogier24: Wat een legendarische avond 52-0 #EKKwalificatie #EKT2015 #LetsGoBelgrado #NederlandLetland pic.twitter.com/oktz0dukwk” #SPECO                                                                                                                        \n",
       "22635   @huismussen goed dat je dat eens geldt                                                                                                                                                                                                                             \n",
       "58414   \" @MounirTweets: Je bent Online Online Online Online Online Online op whatsapp.... Met wie praat je als je niet met mij praat.\"                                                                                                                                    \n",
       "35727   @Politie_Tilburg tip enige vorm van terugkoppeling bij een vraag is niet verkeerd                                                                                                                                                                                  \n",
       "47284   @Leerdenken @haesebroeck @VGZ ja het is echt verschrikkelijk Hoe vaak je formulieren opnieuw moet insturen.                                                                                                                                                        \n",
       "106004  Ik snap wat je bedoeld Veerle, gezamenlijke plannen is ook hier elk jaar weer lastig met een geitenboerin en een vrouw van een agrarisch medewerker als zussen en ik als boer Nogmaals ik vind het niet erg hè verschil Ik wordt niet door een bààs gepiepeld....  \n",
       "41168   @onzetaal @liannekoops vooral leuk omdat zelfs de journalist het woord niet lijkt te kennen Ontdekt door @MaraGelderblom                                                                                                                                           \n",
       "174     Er is op dit moment altijd wel iemand die dans je de hele nacht met mijn in zijn of haar hoofd heeft                                                                                                                                                               \n",
       "8464    #Bathsheba uit Antwerpen met een #Seef uit #antwerpen) rulet roadburnfest culdesactilburg… https://www.instagram.com/p/BTH06mJB2Tn/                                                                                                                                \n",
       "42349   @nynkevdploeg veel plezier daar                                                                                                                                                                                                                                    "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance = 1 # sets the Levenshtein distance for the spellingchecker\n",
    "\n",
    "nl_spell = SpellChecker(language = None, distance = distance)\n",
    "nl_spell.word_frequency.load_dictionary('nl_NL.json')\n",
    "\n",
    "nl_ext = ['ff', \"'s\", 'A2', 'file', 'cda', 'IT', 'CDA', 'mengen', 'kraan', 'NL']\n",
    "nl_spell.word_frequency.load_words(nl_ext)\n",
    "\n",
    "test_nl = temp.loc[temp.loc[:,'langdetect'] == 'nl', :].sample(n = 100, random_state = random_state)\n",
    "\n",
    "test_nl.loc[:, 'corrected'] = test_nl.loc[:,'text'].apply(lambda x: sentence_checker(x, nl_spell))\n",
    "\n",
    "test_nl.loc[:,'gelijk'] = test_nl.loc[:, ['text', 'corrected']].apply(lambda x: x['text'] == x['corrected'], axis = 1)\n",
    "test_nl.loc[test_nl.loc[:,'gelijk'] == False, ['text', 'corrected']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>79729</td>\n",
       "      <td>Morning view from the officepic.twitter.com/dO9ZUk2HPd – at DMG Holding B.V.</td>\n",
       "      <td>Morning view from the officepic.twitter.com/dO9ZUk2HPd a at dog Holding q.v.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100805</td>\n",
       "      <td>facking tasty</td>\n",
       "      <td>facing tasty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19693</td>\n",
       "      <td>Enjoy the beautiful trip and sharing with other enthousiasts. Consider my contribution as a reference to over 50% of EV drivers in NL (together about 70% of elec. miles) that have these Tesla travel benefits on a daily base. Building standards. Will be ok.</td>\n",
       "      <td>Enjoy the beautiful trip and sharing with other enthousiasts. Consider my contribution as a reference to over 50% of EV drivers in NL together about 70% of elect miles that have these tessa travel benefits on a daily based Building standards Will be ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29261</td>\n",
       "      <td>#FlashPoll Tonights movie? #Aquaman Or #MortalEngines</td>\n",
       "      <td>#FlashPoll Tonights movie #Aquaman Or #MortalEngines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2761</td>\n",
       "      <td>With good music comes good food. Best catering on a festival i ever had #goeiepannenkoek… https://www.instagram.com/p/BG2RR1GmfM8/</td>\n",
       "      <td>With good music comes good food Best catering on a festival i ever had #goeiepannenkoek… https://www.instagram.com/p/BG2RR1GmfM8/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23424</td>\n",
       "      <td>After a great weekend with lots of cake, party and qualitytime with the three of us, it's time… https://www.instagram.com/p/BLYnPA8BB7j/</td>\n",
       "      <td>After a great weekend with lots of cake party and qualitytime with the three of us its time https://www.instagram.com/p/BLYnPA8BB7j/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83567</td>\n",
       "      <td>Met de Eftelfans naar @pathe #it #itchapter2 #movies #film #bioscoop @Pathé https://www.instagram.com/p/B2Zu0e8ie2m/?igshid=14zvg5w34kltj …</td>\n",
       "      <td>Met de Eftelfans near pathe it #itchapter2 movies film #bioscoop @Pathé https://www.instagram.com/p/B2Zu0e8ie2m/?igshid=14zvg5w34kltj a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9586</td>\n",
       "      <td>DEATH ALLEY. Raaaawk @Roadburn Festival https://www.instagram.com/p/BEOoHv_KOlh/</td>\n",
       "      <td>DEATH alley Raaaawk @Roadburn Festival https://www.instagram.com/p/BEOoHv_KOlh/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68225</td>\n",
       "      <td>Damnnn I would have loved to see fancy photo's of your sushi and cocktails ^^ But guess you really enjoyed it so thats good too ;)</td>\n",
       "      <td>Damnnn I would have loved to see fancy photos of your sushi and cocktails ^^ But guess you really enjoyed it so thats good too ;)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86561</td>\n",
       "      <td>you didn't even see it coming that fast!</td>\n",
       "      <td>you didn't even see it coming that fast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10546</td>\n",
       "      <td>A2 5041JE 19 : CORNELIS DANCKERTSSTRAAT 19 TILBURG 918 http://watiserloos.in/melding/8088761/ … #p2000</td>\n",
       "      <td>A2 5041JE 19 : cornelius DANCKERTSSTRAAT 19 TILBURG 918 http://watiserloos.in/melding/8088761/ a #p2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43049</td>\n",
       "      <td>“ @wizkhalifa: I'd def still rather be me”</td>\n",
       "      <td>a @wizkhalifa: I'd def still rather be me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50844</td>\n",
       "      <td>Happy Birthday my friend!</td>\n",
       "      <td>Happy Birthday my friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12291</td>\n",
       "      <td>Zon of maan #nofilter #lente #lovesunshine @Tilburg, Netherlands https://www.instagram.com/p/BRiujurh25j/</td>\n",
       "      <td>on of maan #nofilter #lente #lovesunshine @Tilburg, Netherlands https://www.instagram.com/p/BRiujurh25j/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108149</td>\n",
       "      <td>“ @Drakee_YMCMB: Never give up. Great things take time.”</td>\n",
       "      <td>a @Drakee_YMCMB: Never give up Great things take time.”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13559</td>\n",
       "      <td>Durumpje... (@Cafetaria Tropical in Tilburg, Noord-Brabant) https://www.swarmapp.com/c/cGQAQFsKthB</td>\n",
       "      <td>Durumpje... (@Cafetaria Tropical in tilburg Noord-Brabant) https://www.swarmapp.com/c/cGQAQFsKthB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249044</td>\n",
       "      <td>@Actual_NootNoot noot</td>\n",
       "      <td>@Actual_NootNoot not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44237</td>\n",
       "      <td>Net zoals hier! #CarnavalFestivalpic.twitter.com/rjFrgDNI2b</td>\n",
       "      <td>Net goals hiero #CarnavalFestivalpic.twitter.com/rjFrgDNI2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43094</td>\n",
       "      <td>@SensibleMoms yeah, unfortunately. We were driving.There's Reims!</td>\n",
       "      <td>@SensibleMoms yeah unfortunately We were driving.There's reims</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1893</td>\n",
       "      <td>I just finished running 11.72 km in 1h:05m:33s with #Endomondo #endorphins http://goo.gl/cp5Dnx</td>\n",
       "      <td>I just finished running 11.72 km in 1h:05m:33s with #Endomondo endorphins http://goo.gl/cp5Dnx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                    text  \\\n",
       "79729   Morning view from the officepic.twitter.com/dO9ZUk2HPd – at DMG Holding B.V.                                                                                                                                                                                       \n",
       "100805  facking tasty                                                                                                                                                                                                                                                      \n",
       "19693   Enjoy the beautiful trip and sharing with other enthousiasts. Consider my contribution as a reference to over 50% of EV drivers in NL (together about 70% of elec. miles) that have these Tesla travel benefits on a daily base. Building standards. Will be ok.   \n",
       "29261   #FlashPoll Tonights movie? #Aquaman Or #MortalEngines                                                                                                                                                                                                              \n",
       "2761    With good music comes good food. Best catering on a festival i ever had #goeiepannenkoek… https://www.instagram.com/p/BG2RR1GmfM8/                                                                                                                                 \n",
       "23424   After a great weekend with lots of cake, party and qualitytime with the three of us, it's time… https://www.instagram.com/p/BLYnPA8BB7j/                                                                                                                           \n",
       "83567   Met de Eftelfans naar @pathe #it #itchapter2 #movies #film #bioscoop @Pathé https://www.instagram.com/p/B2Zu0e8ie2m/?igshid=14zvg5w34kltj …                                                                                                                        \n",
       "9586    DEATH ALLEY. Raaaawk @Roadburn Festival https://www.instagram.com/p/BEOoHv_KOlh/                                                                                                                                                                                   \n",
       "68225   Damnnn I would have loved to see fancy photo's of your sushi and cocktails ^^ But guess you really enjoyed it so thats good too ;)                                                                                                                                 \n",
       "86561   you didn't even see it coming that fast!                                                                                                                                                                                                                           \n",
       "10546   A2 5041JE 19 : CORNELIS DANCKERTSSTRAAT 19 TILBURG 918 http://watiserloos.in/melding/8088761/ … #p2000                                                                                                                                                             \n",
       "43049   “ @wizkhalifa: I'd def still rather be me”                                                                                                                                                                                                                         \n",
       "50844   Happy Birthday my friend!                                                                                                                                                                                                                                          \n",
       "12291   Zon of maan #nofilter #lente #lovesunshine @Tilburg, Netherlands https://www.instagram.com/p/BRiujurh25j/                                                                                                                                                          \n",
       "108149  “ @Drakee_YMCMB: Never give up. Great things take time.”                                                                                                                                                                                                           \n",
       "13559   Durumpje... (@Cafetaria Tropical in Tilburg, Noord-Brabant) https://www.swarmapp.com/c/cGQAQFsKthB                                                                                                                                                                 \n",
       "249044  @Actual_NootNoot noot                                                                                                                                                                                                                                              \n",
       "44237   Net zoals hier! #CarnavalFestivalpic.twitter.com/rjFrgDNI2b                                                                                                                                                                                                        \n",
       "43094   @SensibleMoms yeah, unfortunately. We were driving.There's Reims!                                                                                                                                                                                                  \n",
       "1893    I just finished running 11.72 km in 1h:05m:33s with #Endomondo #endorphins http://goo.gl/cp5Dnx                                                                                                                                                                    \n",
       "\n",
       "                                                                                                                                                                                                                                                           corrected  \n",
       "79729   Morning view from the officepic.twitter.com/dO9ZUk2HPd a at dog Holding q.v.                                                                                                                                                                                  \n",
       "100805  facing tasty                                                                                                                                                                                                                                                  \n",
       "19693   Enjoy the beautiful trip and sharing with other enthousiasts. Consider my contribution as a reference to over 50% of EV drivers in NL together about 70% of elect miles that have these tessa travel benefits on a daily based Building standards Will be ok  \n",
       "29261   #FlashPoll Tonights movie #Aquaman Or #MortalEngines                                                                                                                                                                                                          \n",
       "2761    With good music comes good food Best catering on a festival i ever had #goeiepannenkoek… https://www.instagram.com/p/BG2RR1GmfM8/                                                                                                                             \n",
       "23424   After a great weekend with lots of cake party and qualitytime with the three of us its time https://www.instagram.com/p/BLYnPA8BB7j/                                                                                                                          \n",
       "83567   Met de Eftelfans near pathe it #itchapter2 movies film #bioscoop @Pathé https://www.instagram.com/p/B2Zu0e8ie2m/?igshid=14zvg5w34kltj a                                                                                                                       \n",
       "9586    DEATH alley Raaaawk @Roadburn Festival https://www.instagram.com/p/BEOoHv_KOlh/                                                                                                                                                                               \n",
       "68225   Damnnn I would have loved to see fancy photos of your sushi and cocktails ^^ But guess you really enjoyed it so thats good too ;)                                                                                                                             \n",
       "86561   you didn't even see it coming that fast                                                                                                                                                                                                                       \n",
       "10546   A2 5041JE 19 : cornelius DANCKERTSSTRAAT 19 TILBURG 918 http://watiserloos.in/melding/8088761/ a #p2000                                                                                                                                                       \n",
       "43049   a @wizkhalifa: I'd def still rather be me                                                                                                                                                                                                                     \n",
       "50844   Happy Birthday my friend                                                                                                                                                                                                                                      \n",
       "12291   on of maan #nofilter #lente #lovesunshine @Tilburg, Netherlands https://www.instagram.com/p/BRiujurh25j/                                                                                                                                                      \n",
       "108149  a @Drakee_YMCMB: Never give up Great things take time.”                                                                                                                                                                                                       \n",
       "13559   Durumpje... (@Cafetaria Tropical in tilburg Noord-Brabant) https://www.swarmapp.com/c/cGQAQFsKthB                                                                                                                                                             \n",
       "249044  @Actual_NootNoot not                                                                                                                                                                                                                                          \n",
       "44237   Net goals hiero #CarnavalFestivalpic.twitter.com/rjFrgDNI2b                                                                                                                                                                                                   \n",
       "43094   @SensibleMoms yeah unfortunately We were driving.There's reims                                                                                                                                                                                                \n",
       "1893    I just finished running 11.72 km in 1h:05m:33s with #Endomondo endorphins http://goo.gl/cp5Dnx                                                                                                                                                                "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting up the english spellingchecker\n",
    "en_spell = SpellChecker(distance = distance)\n",
    "\n",
    "en_ext = [\"i'm\", \"we're\", \"I'll\", \"i'd\", \"you're\"]\n",
    "en_spell.word_frequency.load_words(en_ext)\n",
    "\n",
    "test_en = temp.loc[temp.loc[:,'langdetect'] == 'en', :].sample(n = 100, random_state = random_state)\n",
    "\n",
    "test_en.loc[:, 'corrected'] = test_en.loc[:,'text'].apply(lambda x: sentence_checker(x, en_spell))\n",
    "\n",
    "test_en.loc[:,'gelijk'] = test_en.loc[:, ['text', 'corrected']].apply(lambda x: x['text'] == x['corrected'], axis = 1)\n",
    "test_en.loc[test_en.loc[:,'gelijk'] == False, ['text', 'corrected']].tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing the Twitter data for emotions and sentiment\n",
    "This was originally used to load all data and apply multiple steps at once. Once this was run the data would be stored in 6 pre-processed files. One for each year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first cell does only pre processing. After this step all files are saved to \"tweets pre=processed 2014.csv\". After the previous step the spellingchecker and sentiment analysis can be done. After this step all files are saved to \"tweets+year+.csv\". \n",
    "This was done to not do all the steps again if something went wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year: 2014\n",
      "shape:  (269143, 12)\n",
      "select data\n",
      "Pre-processing\n",
      "Language detection\n",
      "Final shape:  (157227, 13)\n",
      "Finished\n",
      "Time: 2102.5618481636047\n",
      "year: 2015\n",
      "shape:  (80317, 12)\n",
      "select data\n",
      "Pre-processing\n",
      "Language detection\n",
      "Final shape:  (48739, 13)\n",
      "Finished\n",
      "Time: 623.4078094959259\n",
      "year: 2016\n",
      "shape:  (28846, 12)\n",
      "select data\n",
      "Pre-processing\n",
      "Language detection\n",
      "Final shape:  (14528, 13)\n",
      "Finished\n",
      "Time: 193.7040729522705\n",
      "year: 2017\n",
      "shape:  (28251, 12)\n",
      "select data\n",
      "Pre-processing\n",
      "Language detection\n",
      "Final shape:  (11680, 13)\n",
      "Finished\n",
      "Time: 152.8936104774475\n",
      "year: 2018\n",
      "shape:  (25559, 12)\n",
      "select data\n",
      "Pre-processing\n",
      "Language detection\n",
      "Final shape:  (18434, 13)\n",
      "Finished\n",
      "Time: 186.30706906318665\n",
      "year: 2019\n",
      "shape:  (114195, 12)\n",
      "select data\n",
      "Pre-processing\n",
      "Language detection\n",
      "Final shape:  (74959, 13)\n",
      "Finished\n",
      "Time: 739.2866082191467\n"
     ]
    }
   ],
   "source": [
    "### SpellingChecker objects \n",
    "## may take one hour to run\n",
    "distance = 1 # sets the Levenshtein distance for the spellingchecker\n",
    "# setting up the Dutch spellingchecker\n",
    "nl_spell = SpellChecker(language = None, distance = distance)\n",
    "nl_spell.word_frequency.load_dictionary('nl_NL.json')\n",
    "\n",
    "nl_ext = ['ff', \"'s\", 'A2', 'file', 'cda', 'IT', 'CDA', 'mengen', 'kraan', 'NL']\n",
    "nl_spell.word_frequency.load_words(nl_ext)\n",
    "\n",
    "# setting up the english spellingchecker\n",
    "en_spell = SpellChecker(distance = distance)\n",
    "\n",
    "en_ext = [\"i'm\", \"we're\", \"I'll\", \"i'd\", \"you're\"]\n",
    "en_spell.word_frequency.load_words(en_ext)\n",
    "\n",
    "# years of data \n",
    "years = ['2014', '2015', '2016', '2017', '2018', '2019']\n",
    "\n",
    "### Pipeline for Selection, pre-processing, Language detection, spelling checker and Sentiment analysis\n",
    "# for year in years:\n",
    "path = os.getcwd()\n",
    "data_folder = '15 km radius'\n",
    "\n",
    "for year in years:\n",
    "    print('year: '+year)\n",
    "    files = [file for file in os.listdir(path+os.sep+data_folder) if year in file]\n",
    "    temp = pd.DataFrame(data = None, columns = ['date', 'username', 'to', 'replies', 'retweets', 'favorites', 'text',\n",
    "                                                'geo', 'mentions', 'hashtags', 'id', 'permalink'])\n",
    "\n",
    "    for file in files:\n",
    "        temp = temp.append(pd.read_csv(path+os.sep+data_folder+os.sep+file), ignore_index = True)\n",
    "    print('shape: ',temp.shape)\n",
    "    ### # analysis starts here\n",
    "    ### Select data\n",
    "    start = t.time()\n",
    "    print('select data')\n",
    "    tweets = temp\n",
    "    tweets = tweets.drop_duplicates()\n",
    "\n",
    "    p2000 = temp.loc[temp.loc[:,'username'] == 'P2000013', 'id'] # Removing this account, since it only posts emergency calls reportings\n",
    "    tweets = tweets.loc[~tweets.loc[:,'id'].isin(p2000),:]\n",
    "    \n",
    "    ### Pre-processing\n",
    "    print('Pre-processing')\n",
    "    # remove nans in column text\n",
    "    tweets = tweets.dropna(axis = 0, subset = ['text'])\n",
    "    \n",
    "    #remove urls\n",
    "    tweets.loc[:,'text'] = tweets.loc[:,'text'].apply(lambda x: replace_url(x, sub = ''))\n",
    "\n",
    "    # removes tweets without text\n",
    "    tweets.loc[:,'contains text'] = tweets.loc[:,'text'].apply(lambda x: contains_text(x))\n",
    "    tweets = tweets.loc[tweets.loc[:,'contains text'],:]\n",
    "    tweets = tweets.drop(labels = 'contains text', axis= 'columns')\n",
    "\n",
    "    # removes tweets that have less than 3 words\n",
    "    tweets.loc[:,'long'] = tweets.loc[:,'text'].apply(lambda x: small_tweet(x))\n",
    "    tweets = tweets.loc[tweets.loc[:,'long'], :]\n",
    "    tweets = tweets.drop(labels = 'long', axis= 'columns')\n",
    "    \n",
    "    ### language detection\n",
    "    print('Language detection')\n",
    "    tweets.loc[:,'langdetect'] = tweets.loc[:,'text'].apply(lambda x: detect_lang(x))\n",
    "    # only Dutch and English tweets\n",
    "    tweets = tweets.loc[(tweets.loc[:,'langdetect'] == 'nl') | (tweets.loc[:,'langdetect'] == 'en'), :] \n",
    "    print('Final shape: ', tweets.shape)\n",
    "    print('Finished\\nTime: {}'.format(t.time() - start))\n",
    "\n",
    "    tweets.to_csv(data+'tweets pre-processed '+year+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spellingchecker\n",
      "Time: 2.3746187686920166\n",
      "sentiment analysis\n",
      "Time: 11.151150941848755\n",
      "spellingchecker\n",
      "Time: 0.8268132209777832\n",
      "sentiment analysis\n",
      "Time: 6.787846803665161\n",
      "spellingchecker\n",
      "Time: 0.23735737800598145\n",
      "sentiment analysis\n",
      "Time: 4.135928153991699\n",
      "spellingchecker\n",
      "Time: 0.22440361976623535\n",
      "sentiment analysis\n",
      "Time: 1.2496581077575684\n",
      "spellingchecker\n",
      "Time: 0.38795995712280273\n",
      "sentiment analysis\n",
      "Time: 1.1160128116607666\n",
      "spellingchecker\n",
      "Time: 1.7632853984832764\n",
      "sentiment analysis\n",
      "Time: 7.433154582977295\n"
     ]
    }
   ],
   "source": [
    "# tag tweets talking about institutions maually considered \n",
    "institutions = ['#FNV', 'Stadskantoor', 'MWB', 'Burgemeester', 'Burgernet', 'TBV','#MWB', 'beleid', 'gemeentehuis', \n",
    "                'provinciehuis', 'verkiezingen', 'raadslid', '@MVO_NL', 'campagneleider', '@raadtilburg', '@gemeentetilburg', \n",
    "                'minister', 'campagne','bestuursakkoord', '@D66Brabant', 'SP', 'LST', '@GLTilburg', '@SPTilburg', 'VVD', \n",
    "                '@CDATilburg', '@PvdATilburg', '@50pluspartij', '@LokaalTilburg', '#pvv', '@VoorTilburg', '#CDA', \n",
    "                '@Onderzoeksraad', '@IFVtweet', 'debat', '#tilburginbeeld', 'europa', '@D66Tilburg', '#D66Tilburg', \n",
    "                'coalitieakkoord', '@fontys', '@stationTilburg', 'Theresialyceum', '@Brabant', '#Spoorzone', '#lochal', \n",
    "                '#locomotiefhal', '#TilburgU', '#tilburguniversity', '#tiu', '#uvt', '@TilburgUniversity', '#starterslift', \n",
    "                '#topinkomens', '@NatuurmuseumBra', 'cultuurbudget', '@uvt_tilburg', '@BerkelEnschot', '@Avanshogeschool', \n",
    "                'Midpoint', 'BOM', '@BOMBrabant', '@TiwosTilburg', '@WonenBreburg', '@MidpointBrabant', '#midpointbrabant', \n",
    "                '@starterslift', '@nvrewin', '@IFVtweet','regelgeving', '#onderwijs', 'onderwijs', 'debat',\n",
    "                'Europa','#CDA7','#demonstratie', '@minlnv']\n",
    "\n",
    "years = ['2014', '2015', '2016', '2017', '2018', '2019']\n",
    "\n",
    "for year in years:\n",
    "    start = t.time()\n",
    "    tweets = pd.read_csv(data+'tweets pre-processed '+year+'.csv')\n",
    "    tweets.loc[:,'institutions'] = tweets.loc[:,'text'].apply(lambda x: check_inst(x, institutions))\n",
    "\n",
    "    ### spellingchecker\n",
    "    print('spellingchecker\\nTime: {}'.format(t.time() - start))\n",
    "    tweets.loc[(tweets.loc[:,'langdetect'] == 'nl') & (tweets.loc[:,'institutions'] == 1), 'text'] = tweets.loc[(tweets.loc[:,'langdetect'] == 'nl')& (tweets.loc[:,'institutions'] == 1), 'text'].apply(lambda x: sentence_checker(x, nl_spell))\n",
    "    tweets.loc[(tweets.loc[:,'langdetect'] == 'en') & (tweets.loc[:,'institutions'] == 1), 'text'] = tweets.loc[(tweets.loc[:,'langdetect'] == 'en')& (tweets.loc[:,'institutions'] == 1), 'text'].apply(lambda x: sentence_checker(x, en_spell))\n",
    "\n",
    "    ### sentiment analysis\n",
    "    print('sentiment analysis\\nTime: {}'.format(t.time() - start))\n",
    "    tweets.loc[(tweets.loc[:,'langdetect'] == 'nl') & (tweets.loc[:,'institutions'] == 1), 'sentiment'] = tweets.loc[(tweets.loc[:,'langdetect'] == 'nl') & (tweets.loc[:,'institutions'] == 1),'text'].apply(lambda x: nl_sent(x))\n",
    "    tweets.loc[(tweets.loc[:,'langdetect'] == 'en') & (tweets.loc[:,'institutions'] == 1), 'sentiment'] = tweets.loc[(tweets.loc[:,'langdetect'] == 'en') & (tweets.loc[:,'institutions'] == 1),'text'].apply(lambda x: en_sent(x))\n",
    "    tweets.to_csv(data+'tweets '+year+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read tweets pre-processed for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(157227, 15)\n",
      "(205966, 15)\n",
      "(220494, 15)\n",
      "(232174, 15)\n",
      "(250608, 15)\n",
      "(325567, 15)\n",
      "read files\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.DataFrame(data = None, columns = ['date', 'text', 'id'])\n",
    "years = ['2014', '2015', '2016', '2017', '2018', '2019']\n",
    "\n",
    "for year in years:\n",
    "    tweets = tweets.append(pd.read_csv(data+'tweets '+year+'.csv', dtype = {'id':'object'}), ignore_index = False)\n",
    "    print(tweets.shape)\n",
    "print('read files')\n",
    "\n",
    "tweets.loc[:,'date'] = pd.to_datetime(tweets.date, dayfirst = True, infer_datetime_format = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness check of Random wordlist\n",
    "## Sentiment analysis\n",
    "Correlate you monthly sentiment with the entrepreneurial output. \n",
    "Correlate the sentiment in tweets with randomly chosen words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts the occurrence of all words in all tweets so that we can take similar words from this list compared to our own list \n",
    "# of words. \n",
    "# takes about 8 secs # Get the frequency of all words\n",
    "texts_df = pd.DataFrame(tweets.loc[:,'text'].apply(lambda x: re.sub('[\\\\\\'!?,.:\";]', '', x.lower())))\n",
    "word_count = texts_df.loc[:,'text'].str.split(expand=True).stack().value_counts()\n",
    "word_count_df = pd.DataFrame(word_count, columns = ['frequency'])\n",
    "\n",
    "# get the frequency of my own words\n",
    "institutions_freq = {}\n",
    "for key in institutions:\n",
    "    try:\n",
    "        institutions_freq[key] = word_count[key.lower()]\n",
    "    except:\n",
    "        print(key, '-')\n",
    "\n",
    "# Get the random words based on my own list of words with a margin of 5% \n",
    "random.seed(seed = 10)\n",
    "rand_bow = []\n",
    "for word in institutions_freq.keys():\n",
    "    lower, upper = margins(institutions_freq[word])\n",
    "    potential = list(word_count_df.loc[(word_count_df.loc[:,'frequency'] >= lower) & (word_count_df.loc[:,'frequency'] <= upper) ,:].index)\n",
    "    chosen = False\n",
    "    while not(chosen):\n",
    "        chosen_word = potential.pop(random.randint(len(potential)))\n",
    "        if re.search('[0-9-()$%&?!<>/\\\\\\.]', chosen_word):\n",
    "            continue\n",
    "        else:\n",
    "            rand_bow.append(chosen_word)\n",
    "            chosen = True\n",
    "            \n",
    "# put the word list in this list so we have 2 columns which indicates where the tweet belongs to. \n",
    "tweets.loc[:,'random'] = tweets.loc[:,'text'].apply(lambda x: check_inst(x, rand_bow))\n",
    "tweets.loc[:, 'institutions'] = tweets.loc[:,'text'].apply(lambda x: check_inst(x, institutions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.loc[:,['date', 'favorites', 'geo', 'hashtags', 'id', 'institutions','langdetect', 'mentions', 'permalink', \n",
    "                       'replies', 'retweets', 'sentiment', 'text', 'to', 'username', 'random',]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.loc[tweets.loc[:,'langdetect'] == 'nl', 'sentiment'] = tweets.loc[tweets.loc[:,'langdetect'] == 'nl','text'].apply(lambda x: nl_sent(x))\n",
    "tweets.loc[tweets.loc[:,'langdetect'] == 'en', 'sentiment'] = tweets.loc[tweets.loc[:,'langdetect'] == 'en','text'].apply(lambda x: en_sent(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcount of new word list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quaterly and monthly results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.loc[:,'quarter'] = tweets.date.dt.to_period('Q')\n",
    "tweets.loc[:,'month'] = tweets.date.dt.to_period('M')\n",
    "tweets.loc[:,'year'] = tweets.date.dt.to_period('Y')\n",
    "\n",
    "threshold = 0.05\n",
    "\n",
    "tweets.loc[:,'pos'] = tweets.loc[:, 'sentiment'].apply(lambda x: is_pos(x, threshold = threshold))\n",
    "tweets.loc[:,'neg'] = tweets.loc[:, 'sentiment'].apply(lambda x: is_neg(x, threshold = threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A checkpoint for the data to avoid running previous analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################################################\n",
    "\n",
    "tweets.to_pickle(pickles+'tweets 12-6-2020.pkl')\n",
    "\n",
    "#################################################################################################################################\n",
    "# Dataframe with sentiment of all tweets and indication for random words, institutions old and newest word list. Ready to be \n",
    "# calculated in quarter and month and correlate with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_pickle(pickles+'tweets 12-6-2020.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An analysis of the pre-processed tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014    34050\n",
       "2019    15519\n",
       "2015    12539\n",
       "2016    5971 \n",
       "2017    4913 \n",
       "2018    4600 \n",
       "Freq: A-DEC, Name: year, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.loc[tweets.loc[:,'langdetect'] == 'en', 'year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014    123177\n",
       "2019    59440 \n",
       "2015    36200 \n",
       "2018    13834 \n",
       "2016    8557  \n",
       "2017    6767  \n",
       "Freq: A-DEC, Name: year, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.loc[tweets.loc[:,'langdetect'] == 'nl', 'year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_sent(pos, neg):\n",
    "    if pos == 1 or neg == 1:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "tweets.loc[:,'has_sent'] = tweets.loc[:,:].apply(lambda x: has_sent(x['pos'], x['neg']), axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = 'month'\n",
    "monthly_sent = tweets.loc[tweets.loc[:,'institutions'] == 1, [time, 'sentiment', 'pos', 'neg']].groupby(time).mean()\n",
    "has_sent_df = tweets.loc[tweets.loc[:,'institutions'] == 1, [time, 'has_sent']].groupby(time).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tweets.loc[:,['date','id',  'institutions', 'langdetect','sentiment', 'text', 'username', 'random','quarter', 'month','pos','neg', 'has_sent']]\n",
    "temp.to_csv(data+'Final sentiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotions analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "langs = ['en', 'nl']\n",
    "# You have to instll the right lemmatizers in order to lemmatize your text.\n",
    "lemm_dict = {'nl': spacy.load('nl_core_news_sm'), 'en':spacy.load(\"en_core_web_sm\")}\n",
    "\n",
    "text = tweets.loc[:,['text', 'langdetect', 'date', 'id', 'username']]\n",
    "text.loc[:,'old text'] = text.loc[:,'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and lowercase\n",
    "tknzr = TweetTokenizer(preserve_case=False, reduce_len=False, strip_handles=False)\n",
    "text.loc[:, \"text\"] = text.loc[:, \"text\"].apply(lambda x: tknzr.tokenize(x))\n",
    "\n",
    "# remove punctuation\n",
    "punct_to_remove = string.punctuation\n",
    "text.loc[:, \"text\"] = text.loc[:, \"text\"].apply(lambda txt: [x for x in txt if x not in punct_to_remove])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "stop_dict = {'en':stopwords.words('english'), 'nl': stopwords.words('dutch')}\n",
    "for lang in langs:\n",
    "     text.loc[text.loc[:,'langdetect'] == lang, \"text\"] = text.loc[text.loc[:,'langdetect'] == lang, 'text'].apply(lambda txt: [x for x in txt if x not in stop_dict[lang]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translating the emoticons to their textual equivalent\n",
    "vert = {'happy':'blij', 'wink':'knipoog', 'sad':'verdrietig', \n",
    "        'cheeky':'ondeugend', 'crying':'huilen', 'annoyed':'geirriteerd'}\n",
    "emoticons_en = {':)': 'happy', \n",
    "                ';)': 'wink', \n",
    "                ':(': 'sad', \n",
    "                \":p\": 'cheeky', \n",
    "                \";-)\": 'wink', \n",
    "                \":-)\": 'happy', \n",
    "                \":D\": 'happy', \n",
    "                \"(:\":'happy', \n",
    "                \"]:\":'sad', \n",
    "                \":')\":'crying', \n",
    "                ':-/':'annoyed', \n",
    "                ':-p': 'cheeky',\n",
    "                ':-(':'sad', \n",
    "                '):': 'sad', \n",
    "                \":'(\": 'crying'}\n",
    "\n",
    "emoticons_nl = {}\n",
    "for emoticon in emoticons_en.keys():\n",
    "    emoticons_nl[emoticon] = vert[emoticons_en[emoticon]]\n",
    "emoticons = {'nl':emoticons_nl, 'en': emoticons_en}\n",
    "\n",
    "def trans_emoticon(text, emoticon_dict):\n",
    "    '''Changes the emoticon into the word'''\n",
    "    try:\n",
    "        for i, word in enumerate(text):\n",
    "            if word in emoticon_dict.keys():\n",
    "                text[i] = emoticon_dict[word]\n",
    "    except:\n",
    "        print(text)\n",
    "    return text\n",
    "            \n",
    "for lang in langs:\n",
    "    text.loc[text.loc[:,'langdetect'] == lang, \"text\"] = text.loc[text.loc[:,'langdetect'] == lang, \"text\"].apply(lambda txt: trans_emoticon(txt, emoticons[lang]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization --> may take 60 mins\n",
    "def lemmatize(txt, lemmatizer):\n",
    "    return [word.lemma_ for word in lemmatizer(txt)]\n",
    "\n",
    "for lang in langs:\n",
    "    text.loc[text.loc[:,'langdetect'] == lang, \"text\"] = text.loc[text.loc[:,'langdetect'] == lang, \"text\"].apply(lambda txt: lemmatize(' '.join(txt), lemm_dict[lang]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the emotion lexicon\n",
    "derived emotions:\n",
    "- dominant emotion = \n",
    "    - valence approach -> what is the most frequent emotion positive or negative\n",
    "    - cognitive appraisal -> what is the most frequent high or low controllability and certainty\n",
    "- conflicting emotion \n",
    "    - valence approach -> what is the least frequent emotion positive or negative\n",
    "    - cognitive appraisal -> what is the least frequent high or low controllability and certainty\n",
    "- mixed emotions (C --> Conflicting, D --> Dominant)\n",
    "    - $$5*(C+1)^p - (D+1)^{1/C}$$ --> p is less than 1 (0.5) and \n",
    "    \n",
    "| emotion| Valence| Controlability  | Certainty|\n",
    "| - |-| -|-|\n",
    "| Joy (Happiness) | positive | High | High |\n",
    "| Fear (fear) | Negative | Low | Low |\n",
    "| Anticipation (hope) | Positive | Low | Low |\n",
    "| Anger (anger) | Negative | High | High |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint in between again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.to_pickle(pickles+'pre-processed.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_pickle(pickles+'pre-processed.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotions are detected in the text\n",
    "emo_lex = pd.read_csv('NRC/NRC-Emotion-Lexicon-v0.92/NRC-Emotion-Lexicon-v0.92-In105Languages-Nov2017Translations - corrected.csv', encoding = 'iso8859_15')\n",
    "\n",
    "fear_nl = list(emo_lex.loc[emo_lex.loc[:,'Fear'] == 1, 'dutch new'])\n",
    "anger_nl = list(emo_lex.loc[emo_lex.loc[:,'Anger'] == 1, 'dutch new'])\n",
    "Surprise_nl = list(emo_lex.loc[emo_lex.loc[:,'Surprise'] == 1, 'dutch new'])\n",
    "joy_nl = list(emo_lex.loc[emo_lex.loc[:,'Joy'] == 1, 'dutch new'])\n",
    "fear_en = list(emo_lex.loc[emo_lex.loc[:,'Fear'] == 1, 'English (en)'])\n",
    "anger_en = list(emo_lex.loc[emo_lex.loc[:,'Anger'] == 1, 'English (en)'])\n",
    "Surprise_en = list(emo_lex.loc[emo_lex.loc[:,'Surprise'] == 1, 'English (en)'])\n",
    "joy_en = list(emo_lex.loc[emo_lex.loc[:,'Joy'] == 1, 'English (en)'])\n",
    "\n",
    "emo_lex_dict = {'nl': {'Fear': fear_nl, 'Anger': anger_nl, 'Surprise': Surprise_nl, 'Joy': joy_nl},\n",
    "                'en': {'Fear': fear_en, 'Anger': anger_en, 'Surprise': Surprise_en, 'Joy': joy_en}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "532"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_lex.loc[(emo_lex.loc[:, 'Surprise'] == 1) & (emo_lex.loc[:, 'Surprise'] == 1), :].count()\n",
    "emo_lex.loc[(emo_lex.loc[:, 'Surprise'] == 1),'English (en)'].count()\n",
    "emo_lex.loc[(emo_lex.loc[:, 'Surprise'] == 1),'English (en)'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nl\n",
      "Fear\n",
      "Anger\n",
      "Surprise\n",
      "Joy\n",
      "en\n",
      "Fear\n",
      "Anger\n",
      "Surprise\n",
      "Joy\n"
     ]
    }
   ],
   "source": [
    "def find_emo(txt, emo_lookup):\n",
    "    '''\n",
    "    counts the occurence of each emotion in the text\n",
    "    txt: a list of words\n",
    "    emo_lookup: a dataframe where words can be looked up in the index\n",
    "    returns: a list with all emotions added up\n",
    "    '''\n",
    "    if type(emo_lookup) != type(list()):\n",
    "        raise TypeError('\"emo_lookup\" should be type: list')\n",
    "    emo_count = 0\n",
    "    for word in txt:\n",
    "        if word in emo_lookup:\n",
    "            emo_count += 1\n",
    "    \n",
    "    return emo_count\n",
    "\n",
    "langs = ['nl', 'en']\n",
    "        \n",
    "for lang in langs:\n",
    "    print(lang)\n",
    "    for emo in emo_lex_dict[lang].keys():\n",
    "        print(emo)\n",
    "        text.loc[text.loc[:, 'langdetect'] == lang,emo] = text.loc[text.loc[:, 'langdetect'] == lang,'text'].apply(lambda txt: find_emo(txt, emo_lex_dict[lang][emo]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.loc[:, ['text', 'langdetect', 'date', 'username', 'Fear', 'Anger', 'Joy', 'Surprise']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.to_csv(data+'Final emotion.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
